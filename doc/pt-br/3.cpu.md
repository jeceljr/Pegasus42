- [1. Chaves, Portas Lógicas, Circuitos Combinacionais](1.comb.md)
- [2. Circuitos Sequenciais](2.seq.md)

---

# 3. Processadores

Todos os problemas computacionais podem ser resolvidos com uma FSM. Teoricamente. Na prática até problemas
relativamente reduzidos podem precisar de um número de estados absurdo e a máquina correspondente seria
impossivelmente cara para se construir (se não absolutamente impossível, se precisa de mais componentes
do que existem átomos no universo, por exemplo).

Imagine uma FSM para receber seis characteres de 7 bits cada cuja função é imprimir estes caracteres na
ordem invertida. Ela vai ter 8_865_353_597_185 (8 trilhões) de estados. O problema é que a única memória
do sistema é o registrador do estado atual e usar isso para guardar que caracteres já forma vistos não é
eficiente. 6x7 = 42 bits para guardar os caracteres enquanto 8 trihões de estados precisam de 44 bits
para representá-los.

Se usarmos uma FSM ligada a uma pequena memória externa o resultado seria bem melhor. Uma memória de
8 palavras de 8 bits cada estaria sobrando e uma FSM com 12 estados seria suficiente para controlá-la
para resolver o problema.

Em seu trabalho de 1936 o Alan Turing imaginou also ainda mais simples que uma memória: ele ligou a
FSM dele (que representou como tabelas no texto) a uma fita infinita com casa individuais que podem
conter um símbolo escolhido entre um certo alfabeto. Existe uma cabeça de leitura e
gravação que está posicionada numa das casas da fita. A entrada da FSM é o símbolo da casa atual da
fita e as saídas são um símbolo a ser gravado (possivelmente o mesmo se não quisermos alterar a fita
neste momento) e opcionalmente um comando para mover a cabeça para a casa da esquerda o da direita.

Hoje isso é conhecido como "Máquina de Turing". Um simulador muito interessante disponível na web
é (https://turingmachine.io/) que já inclue vários exemplos.

![simulador de Máquinas de Turing](../fig/3.00.turingmachine.io.png)

Aqui temos uma multiplicação de dois números binários. A mesma coisa como uma FSM pura teria um
número enorme de estados enquanto aqui só foram necessários 21 estados. Mas isto foi criado apenas
para estudar um problema matemático do tipo "existe uma Máquina de Turing capaz de ..." e não
ser algo prático. Cada problema requer a construção de uma Máquina de Turing própria, mas no final
do texto aparece uma proposta interessante: uma Máquina Universal de Turing que receberia na mesma
fita os dados de entrada e uma representação, na forma de uma sequência de símbolos, de
uma Máquina de Turing que resolva o problema desejado. Hoje chamamos isso de "interpretador". Uma
vez construida esta máquina, trocando a fita muda o funcionamento. É o que chamamos de "software".

## Arquitetura de von Neumann e de Harvard

O primeiro computador (ou "cérebro eletrônico" conforme a imprensa da época) divulgado para o público
foi o Eniac em 1946 (projetos anteriores foram mantidos em segredo por muitos anos). Projetado
por John Mauchly e J. Presper Eckert  na Universidade da Pennsylvania, a principal limitação do
ENIAC era a necessidade de reconfigurar o hardware via paineis de fios para cada novo problema. Então
mesmo durante o seu desenvolvimento eles passaram a discutir o sucessor, a ser chamado de EDVAC.

Um dos participantes destes debates era o John von Neumann e ele escreveu um relatório detalhado
com estas idéias. Outro participante, o Herman Goldstine, acabou distribuindo o relatório para grupos
externos com apenas o von Neumann como autor, por isso este estilo de computação é conhecido como
"arquitetura von Neumann" apesar de ter sido criada por um grupo.

![arquitetura von Neumann](../fig/3.01.vonneumann.svg)

O John von Neumann gostava de analogias com a biologia e por isso chamava as partes do computador de "orgãos"
e onde os dados ficavam de "memória" (outros, especialmente a IBM, preferiam termos como "armazenagem"
mas acabaram perdendo esta batalha).

A únidade de control é uma FSM, que já vimos. A unidade lógica e aritmética é uma versão mais complicada
so somador/subtrator que também já vimos. A entrada e a saída é diferente para cada computador, então vamos
ignorá-los por enquanto.

A memória guarda dados e os programas (como a fita da Máquina Universal de Turing). Um bit desta memória é
como o registrador, que já vimos. O que não falamos é como escolher um bit entre muitos, mas o multiplexador
é bem parecido com o mecanísmo usado. Um aspecto interessante da arquitetura de von Neumann é que o
processador central (CPU) pode ser feito com uma tecnologia completamente diferente da usar pela memória.
Alguns exemplos:

| Computador | CPU | Memória |
|------------|-----|---------|
| EDSAC | válvulas | tanques de mercúrio |
| IAS | válvulas | tubos de Williams |
| LGP-30 | válvulas | tambor magnético |
| PDP-8 | transistores | núclos de ferrite |
| PC moderno | chip digitais | chip com capacitores verticais |

O IAS é "Institute for Advanced Studies" de Princeton onde John von Neumann e equipe construiram o computador
do seu relatório. Por isso também é mais raramente conhecido como "arquitetura de Princeton". Isso é em
contraste com "arquitetura de Harvard" cujo nome é baseado no computador que a IBM construiu para Howard Aiken da
Universidade de Harvard. A única diferença é que a memória de dados e a memória de programas são separadas. A
separação permite uma instrução e um dado serem lidos ao mesmo tempo, o que facilita o projeto. Mas impede
que um programa modifique a si mesmo.

Hoje todos os computadores menos os mais simples são hibridos: diretamente no processador temos duas pequenas
memórias conhecidas como cache de instrução de nível 1 e cache de dados de nível 1. Estas memórias apenas guardam
cópias das informações mais recentes usadas pelo computador. Quando a informação desejada não está nelas, um
cache unificado de nível 2 é acessado. E pode existir um nível 3 de cache e finalmente chegamos à memória
principal única como na figura do von Neumann. Isto combina as vantagens de hardware da arquitetura Harvard
com as vantagens de programação da arquitetura von Neumann.

Mencionamos que as entradas e saídas são específicas para cada computador. Nos primeiros computadores isso se
refletia no conjunto de instruções. Um computador podia ter uma instrução para ler uma tecla e outra instrução
para gravar numa fita. Por volta de 1970 começou a ficar popular a idéia de fazer os dispositivos aparecerem
como posições especiais de memórias. Ai o processador podia usar as instruções "normais" para tudo. Dos
processadores mais usados, apenas o x86 (Intel e AMD) ainda usam instruções especiais para entrada e saída
e os outros usam "periféricos mapeados na memória", o que nós também faremos.

## MCPU16h

---

- [4. FPGAs e Shin JAMMA](4.fpga.md)
- [5. Vídeo e Áudio](5.av.md)
- [6. Pegasus 42](6.pegasus42.md)
- [A. História](A.hist.md)
