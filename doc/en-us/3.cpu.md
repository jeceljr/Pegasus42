- [1. Switches, Logic Gates, Combinational Circuits](1.comb.md)
- [2. Sequential Circuits](2.seq.md)

---

# 3. Processors

All computational problems can be solved with a FSM. In theory. In practice even relatively
small problems might need an absurd number of states and the corresponding machine would be
impossibly expensive to build (if not absolutely impossible, for example if it needs more
components than there are atoms in the universe).

Imagine a FSM that receives six characters of 7 bits each and which is supposed to print thesse
characters in the reverse order. It will have 8_865_353_597_185 (8 trillion) states. The problem
is that the system's only memory is the register for the current state and using that to store
what characters have already been seen is not efficient. 6x7 = 42 bits to store the characters
while 8 trillion states need 44 bits to represent them.

If we use a FSM connected to a small external memory it would be much more reasonable. A memory
with 8 words of 8 bits each would be more than enough and a FSM with 12 state would be sufficient
to control it to solve the problem.

In his 1936 paper, Alan Turing imagined something even simpler than a memory: he connected his
FSM (represented in the paper as a table) to an inifinite tape with individual cells that can
hold a single symbol chosen from some alphabet. There is a read/write head that is position on
one of the tape's cells. The input to the FSM is the symbol in the current cell and the outputs
are a symbol to be written (possibly the same one of we don't wish to change the tape at this
time) and optionally a command to move the head to the cell on the left or on the right.

Today this is known as a "Turing Machine". A very interesting simulator which is available on
the web is (https://turingmachine.io/) which includes several examples.

![Turing Machine simulator](../fig/3.00.turingmachine.io.png)

Here we have the multiplication of two binary numbers. The same thing as a pure FSM would have
a much larger number of state, while here only 21 are needed. But this was created only to
study a mathematical problem of the kind "there exists a uring machine capable of..." and not
to be something practical. Each problem requires the construction of a new Turing Machine, but
towards the end of the paper there is a very interesting proposal: a Universal Turing machine
that would receive on the same tape as the input data a representation, in the form of a
sequence of symbols, of a Turing Machine that would solve the selected problem. We now call this
an "interpreter". Once such a machine was built, changing the tape would modify its behavior.
This is what we call "software".

## von Neumann and Harvard Architectures

The first computer (or "electronic brain" as the press inicially called them) was ENIAC in 1946
(previous projects were kept secret for many years). Designed by John Mauchly e J. Presper Eckert
at the University of Pennsylvania, a key limitation of the ENIAC was the need to reconfigura the
hardware using patch cable panels for each new problem. So even during its development they
started discussing its sucessor, to be called EDVAC.

One of the participants of these debates was John von Neumann and he wrote a detailed report on
these ideas. Another participant, Herman Goldstine, ended up distributing the report to several
external grupos with von Neumann as the sole author, so this style of computing is known as the
"von Neumann architecture" even though it was created by a group.

![von Neumann architecture](../fig/3.01.vonneumann.svg)

John von Neumann liked analogies with biology and so called the parts of the computers "organs"
and where the data is kept "memory" (others, specially IBM, prefered terms such as "storage" but
ended up losing this battle).

The control unit is just a FSM, which we have already seen. The arithmetic and logic unit is a
more complicated version of the adder/subtractor which we have also already seen. The input and
output are different for each computer, so for now we will ignore them.

The memory stores both data and programas (just like the tape of the Universal Turing Machine). One
bit of this memory is like a register, which we have already seen. What we didn't talk about is how
to select one bit from many, but the multiplexer is similar to the mechanism used. An interesting
feature of the von Neumann architecture is that the central processor (CPU) can be made from
a technology that is completely different from that used to make memory. Some examples:

| Computer | CPU | Memory |
|----------|-----|--------|
| EDSAC | vacuum tubes | mercury tanks |
| IAS | vacuum tubes | Williams tubes |
| LGP-30 | vacuum tubes | magnetic drum |
| PDP-8 | transistors | magnetic cores |
| modern PC | digital chips | chips with vertical capacitors |

IAS is "Institute for Advanced Studies" in Princeton where John von Neumann and his team built the
computer from his report. That is why it is more rarely referred to as the "Princeton architecture".
That is to contrast it with the "Harvard architecture", which is named after a computer that IBM
built for Howard Aiken at Harvard University. The only difference is that the data memory and program
memory are separate. The separation allows an instruction and some data to be read at the same time,
which can simplify the project. But makes it impossible for a program to modify itself.

Today all computers except for the simplest ones are hybrids: directly connected to the processor we
have two tiny memories known as the level 1 instruction cache and the level 1 data cache. These
memories only store copies of information that have been recently used by the computer. When the needed
information is not there, a level 2 unified cache is acessed. And there might b e a level 3 cache and
then finally the main memory as in von Neumann's drawing. This combines the hardware advantages of the
Harvard architecture with the programming advantages of the von Neumann architecture.

We mentioned that inputs and outputs are specific to each computer. In early computers this was reflected
in the instruction set. A computer might have one intruction to read a key and another instruction to
write to the tape. Around 1970 the idea of making input/output devices appear as special memory locations
began to become popular. This allows the processor to use "normal" instructions for everything. Of the
processors current used, only x86 (Intel and AMD) still have special instructions for input and output
and all the users use "memory mapped peripherals", which we will do as well.

## MCPU16h

---

- [4. FPGAs and Shin JAMMA](4.fpga.md)
- [5. Video and Audio](5.av.md)
- [6. Pegasus 42](6.pegasus42.md)
- [A. History](A.hist.md)
