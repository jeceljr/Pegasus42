- [1. Switches, Logic Gates, Combinational Circuits](1.comb.md)
- [2. Sequential Circuits](2.seq.md)

---

# 3. Processors

All computational problems can be solved with a FSM. In theory. In practice even relatively
small problems might need an absurd number of states and the corresponding machine would be
impossibly expensive to build (if not absolutely impossible, for example if it needs more
components than there are atoms in the universe).

Imagine a FSM that receives six characters of 7 bits each and which is supposed to print thesse
characters in the reverse order. It will have 8_865_353_597_185 (8 trillion) states. The problem
is that the system's only memory is the register for the current state and using that to store
what characters have already been seen is not efficient. 6x7 = 42 bits to store the characters
while 8 trillion states need 44 bits to represent them.

If we use a FSM connected to a small external memory it would be much more reasonable. A memory
with 8 words of 8 bits each would be more than enough and a FSM with 12 state would be sufficient
to control it to solve the problem.

In his 1936 paper, Alan Turing imagined something even simpler than a memory: he connected his
FSM (represented in the paper as a table) to an inifinite tape with individual cells that can
hold a single symbol chosen from some alphabet. There is a read/write head that is position on
one of the tape's cells. The input to the FSM is the symbol in the current cell and the outputs
are a symbol to be written (possibly the same one of we don't wish to change the tape at this
time) and optionally a command to move the head to the cell on the left or on the right.

Today this is known as a "Turing Machine". A very interesting simulator which is available on
the web is (https://turingmachine.io/) which includes several examples.

![Turing Machine simulator](../fig/3.00.turingmachine.io.png)

Here we have the multiplication of two binary numbers. The same thing as a pure FSM would have
a much larger number of state, while here only 21 are needed. But this was created only to
study a mathematical problem of the kind "there exists a uring machine capable of..." and not
to be something practical. Each problem requires the construction of a new Turing Machine, but
towards the end of the paper there is a very interesting proposal: a Universal Turing machine
that would receive on the same tape as the input data a representation, in the form of a
sequence of symbols, of a Turing Machine that would solve the selected problem. We now call this
an "interpreter". Once such a machine was built, changing the tape would modify its behavior.
This is what we call "software".

## von Neumann and Harvard Architectures

The first computer (or "electronic brain" as the press inicially called them) was ENIAC in 1946
(previous projects were kept secret for many years). Designed by John Mauchly e J. Presper Eckert
at the University of Pennsylvania, a key limitation of the ENIAC was the need to reconfigura the
hardware using patch cable panels for each new problem. So even during its development they
started discussing its sucessor, to be called EDVAC.

One of the participants of these debates was John von Neumann and he wrote a detailed report on
these ideas. Another participant, Herman Goldstine, ended up distributing the report to several
external grupos with von Neumann as the sole author, so this style of computing is known as the
"von Neumann architecture" even though it was created by a group.

![von Neumann architecture](../fig/3.01.vonneumann.svg)

John von Neumann liked analogies with biology and so called the parts of the computers "organs"
and where the data is kept "memory" (others, specially IBM, prefered terms such as "storage" but
ended up losing this battle).

The control unit is just a FSM, which we have already seen. The arithmetic and logic unit is a
more complicated version of the adder/subtractor which we have also already seen. The input and
output are different for each computer, so for now we will ignore them.

The memory stores both data and programas (just like the tape of the Universal Turing Machine). One
bit of this memory is like a register, which we have already seen. What we didn't talk about is how
to select one bit from many, but the multiplexer is similar to the mechanism used. An interesting
feature of the von Neumann architecture is that the central processor (CPU) can be made from
a technology that is completely different from that used to make memory. Some examples:

| Computer | CPU | Memory |
|----------|-----|--------|
| EDSAC | vacuum tubes | mercury tanks |
| IAS | vacuum tubes | Williams tubes |
| LGP-30 | vacuum tubes | magnetic drum |
| PDP-8 | transistors | magnetic cores |
| modern PC | digital chips | chips with vertical capacitors |

IAS is "Institute for Advanced Studies" in Princeton where John von Neumann and his team built the
computer from his report. That is why it is more rarely referred to as the "Princeton architecture".
That is to contrast it with the "Harvard architecture", which is named after a computer that IBM
built for Howard Aiken at Harvard University. The only difference is that the data memory and program
memory are separate. The separation allows an instruction and some data to be read at the same time,
which can simplify the project. But makes it impossible for a program to modify itself.

Today all computers except for the simplest ones are hybrids: directly connected to the processor we
have two tiny memories known as the level 1 instruction cache and the level 1 data cache. These
memories only store copies of information that have been recently used by the computer. When the needed
information is not there, a level 2 unified cache is acessed. And there might b e a level 3 cache and
then finally the main memory as in von Neumann's drawing. This combines the hardware advantages of the
Harvard architecture with the programming advantages of the von Neumann architecture.

We mentioned that inputs and outputs are specific to each computer. In early computers this was reflected
in the instruction set. A computer might have one intruction to read a key and another instruction to
write to the tape. Around 1970 the idea of making input/output devices appear as special memory locations
began to become popular. This allows the processor to use "normal" instructions for everything. Of the
processors current used, only x86 (Intel and AMD) still have special instructions for input and output
and all the users use "memory mapped peripherals", which we will do as well.

## MCPU16h

In the EDAVC report the idea was put forth  of representing a program as a series numbers where the most
significant bits would represent an "order" being given to the computer (this is now called the "operation
code" or just "opcode) and the least significant bits would be the address of the memory position to be
used for this instruction. Many operations need two operands and produce a result that must be stored
somewhere. This would require 3 addresses, mas EDVAC inherited from mechanical calculators the idea
of a special register called "accumulator" which supplies one of the operands and is the destination of
the result for most of the instructions.

How many different instructions will we have? This defines how many bits we will need for the "opcode". It
turns out that it is actually possible to do everything with just a single instruction, but that is not
a good idea from an educational viewpoint. An example of such an instruction is SUBLEQ which has 3 addresses
and subtracts the value at the first address from what is at the second address (saving the result back there)
and if the result was negative or equal it jumps to the third address. Programas written for such a computer
are nearly as hard to understand as those for a Turing Machine.

A more reasonable alternative is [MCPU](https://github.com/cpldcpu/MCPU) with its 4 instructions (two bits
for the "opcode"). Tim Böscke was inspired by MPROZ and its 3 instructions, but replacing the memory to
memory operations with an accumulator in the EDVAC style greatly simplified the project.

MCPU16h has two differences: while the original MCPU is 8 bits wide so only 6 bits are left for the address
(pnly 64 bytes, which is sufficient for the simplest examples) MCPU16h, as its name implies, is 16 bits
wide and its 14 bit addresses can handle 16 thousand words of 16 bits each (32KB). And the "h" at the end
of the name is from Harvard. But like the EDVAC, MCPU16h depends on being able to modify a program while it
is running but that is not possible with a Harvard architecture. But *Digital* has a two port memory and
we will use that to make it look like there are two separate memories but what is written to one can be
read by the other.

The choice of the Harvard architecture was to allow the use of a combinational circuit for the control unit.
In a von Neumann architecture (like the original MCPU) reading an instruction happens in one cycle while
reading data in a different one. Because of this the control unit has to be a sequential circuit, which
is a bit more complex.

![PC do MCPU](../fig/3.02.mcpu_pc.svg)

With this simple circuit, at each rising clock edge the PC advances to the next word. In this case we
connected the "enable" of the PC register to 1 since we never stop changing the PC. But in a more
complete project there will be situations where that will happen (if we have to wait for the instruction
memory, for example).

So let's add the dual port memory. Port 2 is limited to reading and will be used as the instruction memory.
The control signals for port 1 are connected to constant values that won't interfere with the operation
(not writing anything, for example).

![dual port memory](../fig/3.03.mcpu_mem.svg)

When simulation starts the memory is full of 0s. Later on we will use the contents from a file to fill
in the memory, but for not we are manually editing the values of the first few words every time the
simulation starts just to see that something is actually happening. The picture shows what happens after
two clock rising edges.

The data arriving from memory (0x7777 indicating that it is the hexadecimal equivalent to the binary 0111011101110111)
is divided into a 14 bit address (0x3777) and two "opcode" bits (0 and 1). We are decoding the instructions using
only wires.

We shall implement the first of the four instructions: JCC ("Jumps if Carry Clear"). We need to choose an
"opcode" for this instruction and we selected 1 and 1.

![JCC instruction](../fig/3.04.mcpu_jcc.svg)

The multiplexer decides between jumping to the address indicated by the instruction or just adding 1 to the current PC.
The first case should only happen if the opcode is that of the JCC and if the *C* bit is 0. The 3 input AND gate
detects this situation and controls the multiplexer.

Here we are creating a single circuit, but normally the AND gate would be part of the control unit while the
multiplexer, the PC register and the adder would be part of what we call the "data path". And the memory would
be separate from the CPU.
```
clock C PC
program(0x0000, 0xC006, 0xC001, 0xC008)
0 1 0
C 1 1
C 1 2
C 0 1
C 1 2
C 0 1
C 0 6
```
To test this circuit we need to make PC an output so it can be compared during the test. We initialize the first
4 words of memory with a tiny test program. Instruction 0 will be ignored for now and instructions 1, 2 and 3
are all JCC. The first time that instruction 1 is executed (line 5 of the test) *C* é 1 and PC is incremented
to 2. In lines 7 and 9 instruction 1 is executed againm with *C* equal to 1 and *C* equal to 0 respectively.
Only in the last case it doesn't go on to 2, but instead jumps to 6.

![JCC and NOR instructions](../fig/3.05.mcpu_jcc_nor.svg)

The next instruction we will implement is NOR, for which we chose the opcode 0 and 0. We need a new 16 bit
register that we will call Acc. We will also have an output with this name to be able to use it in the tests.
The 16 bit wide NOR gets one input from Acc and the other from memory, with the result going back to Acc.

First we test JCC to check that it continues to work. Then we create a new test for NOR. The circuit must
pass both.
```
clock Acc
program(0x0004, 0x0000, 0x0001, 0x0002, 0xFFFF)
0 0
C 0
C 0xFFFB
C 4
C 0xFFFA
```
Curiously, words 0, 1 and 2 are initially used as instructions but then are used as data.

The STA instruction ("STore Accumulator") will have the opcode of 1 and 0. Acc's output will be the data
input for the memory (which was always 0 up to now). And the memory's *str* signal will be activated
by this instruction, writing to the indicated address.

![JCC, NOR and STA instructions](../fig/3.06.mcpu_jcc_nor_sta.svg)

After checking that the JCC and NOR tests continue to work, we create a new test for STA.
```
clock Acc
program(0x0004,0x8004,0xC000,0x0000,0xFFFF)
0 0
C 0
C 0
C 0
C 0xFFFF
C 0xFFFF
C 0xFFFF
C 0
C 0
C 0
C 0xFFFF
```
It isn't possible to test STA without also using other instructions. The results of writing data is
only visible when you read back that data and for that we need NOR. The test also uses JCC just to
keep the code short - a sequence of NOR, STA, NOR, STA, NOR... would be good enough test.

The last instruction, ADD, would be nearly the same as NOR if it wasn't the complication of the carry
bit. But as both ADD and NOR write to Acc we need a multiplexer to decide between them.

![the complete mcpu](../fig/3.07.mcpu.svg)

Now Acc needs to be enabled both for ADD (opcode 01) as well as NOR (opcode 00), so we can replace the
AND gate with the two inverted inputs with a simple inverter. The new adder has the same inputs as the NOR.

The *C* input is replaced by a 1 bit wide *C* register. This forces us to change the JCC test. The
definition of JCC is that *C* is cleared independently of whether the jump happens or not. This is used
so two JCC in a row will be an unconditional jump (which in other processors is a separate instruction).
The carry output of the adder is forced to 0 in the case of a JCC instruction (or a STA, but in that
case it doesn't matter).

*C* needs to be enabled for the ADD (opcode 01) and JCC (opcode 11) instructions, so the lower bit of
the opcode can be used directly for that.

With that, MCPU is complete and can execute complex programs.

---

- [4. FPGAs and Shin JAMMA](4.fpga.md)
- [5. Video and Audio](5.av.md)
- [6. Pegasus 42](6.pegasus42.md)
- [A. History](A.hist.md)
